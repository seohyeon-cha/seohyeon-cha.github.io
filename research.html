---
layout: page
title: Research
---

<h2> Current Work </h2>
<p>
	My main focus is conformal prediction with Bayesian models, especially in graph dataset. 
</p>

<div class="project">
	<div class="project-thumb"><img src="/photos/temp.png" alt="Empirical coverage and inefficiency of Bayesian CP"></div>
	<div class="project-content">
		<h3>On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction</h3>
		<strong>Seohyeon Cha</strong>, <span style="color:#767676">Honggu Kang, Joonhyuk Kang<br>
		Under-review.
		<div>
		<a id="show_cha2023_temp" href="#show_cha2023_temp" class="hide">abstract (+)</a>
		<a id="hide_cha2023_temp" href="#hide_cha2023_temp" class="show">abstract (-)</a>
		<p class="details">
			Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing valid prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as inefficiency, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is well-calibrated only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration. 
		</p>
		</div>
	</div>
</div>


<div class="project">
	<div class="project-thumb"><img src="/photos/nefl.png" alt="stroke subject demonstrating hand closing and opening while wearing device"></div>
	<div class="project-content">
		<h3>NeFL: Nested Federated Learning for Heterogeneous Clients</h3>
		<span style="color:#767676">Honggu Kang, </span><strong>Seohyeon Cha</strong><span style="color:#767676">, Jinwoo Shin, Jongmyeong Lee, and Joonhyuk Kang</span><br>
		Under-review. 
		<div>
		<a href="https://arxiv.org/pdf/2308.07761.pdf">paper</a> / 
		<a id="show_cha2023_nefl" href="#show_cha2023_nefl" class="hide">abstract (+)</a>
		<a id="hide_cha2023_nefl" href="#hide_cha2023_nefl" class="show">abstract (-)</a>
		<p class="details">
			Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33% improvement on CIFAR-10). Furthermore, we demonstrate NeFL aligns with recent studies in FL.
		</p>
		</div>
	</div>
</div>

<h2> Previous Work </h2>


<div class="project">
	<div class="project-thumb"><img src="/photos/its.png" alt="stroke subject demonstrating hand closing and opening while wearing device"></div>
	<div class="project-content">
		<h3>Intelligent Surface-aided Transmitarray Antenna in mmWave Communication System with Historical Channel Observation</h3>
		<strong>Seohyeon Cha</strong><span style="color:#767676">, Sanghyuk Kim, Jiwan Seo, and Joonhyuk Kang</span><br>
		In IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia), 2022. 
		<div>
		<a href="https://ieeexplore.ieee.org/document/9954733">paper</a> / 
		<a id="show_cha2022_its" href="#show_cha2022_its" class="hide">abstract (+)</a>
		<a id="hide_cha2022_its" href="#hide_cha2022_its" class="show">abstract (-)</a>
		<p class="details">
			In this paper, we study an intelligent surface-aided transmit-array antenna architecture which deploys intelligent transmitting surface (ITS) near the active antenna in mmWave downlink communication system. We aim to maximize the average achievable rate at the user by optimizing the phase shift matrix of ITS. To overcome the difficulty of acquiring channel state information (CSI), we propose a stochastic gradient descent (SGD)-based algorithm that requires only historical channel observations. Simulation results show that our proposed scheme with single active antenna and ITS outperforms conventional MISO system.
		</p>
		</div>
	</div>
</div>


<footer class="footer" style="vertical-align:bottom;text-align:left;margin-top:50px;">
	<hr>
    <div>kaitjgus<span style="font-size:9pt;padding-right:1px;padding-left:1px;"><i alt="at sign" class="fas fa-at"></i></span>kaist.ac.kr</div>
    <div>Last Updated:&nbsp; <time datetime="{{ site.time | date_to_xmlschema }}">{{ site.time | date: '%m-%d-%Y' }}</time></div>    
</footer>
