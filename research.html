---
layout: page
title: Research
---

<h2> Conformal Prediction </h2>
<p>
	My main focus is conformal prediction with Bayesian models, especially in graph dataset. 
</p>

<div class="project">
<!-- 	<div class="project-thumb"><img src="/photos/thumb.jpg" alt="stroke subject demonstrating hand closing and opening while wearing device"></div> -->
	<div class="project-content">
		<h3>NeFL: Nested Federated Learning for Heterogeneous Clients</h3>
		<span style="color:#767676">Honggu Kang, </span><strong>Seohyeon Cha</strong><span style="color:#767676">Jinwoo Shin, Jongmyeong Lee, and Joonhyuk Kang</span><cbr>
		Under-review in NeurIPS 2023. 
		<div>
		<a href="https://openreview.net/pdf?id=hldhaq2P2z">paper</a> / 
		<a id="show_cha2023_nefl" href="#show_cha2023_nefl" class="hide">abstract (+)</a>
		<a id="hide_cha2023_nefl" href="#hide_cha2023_nefl" class="show">abstract (-)</a>
		<p class="details">
			Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.01% improvement on CIFAR-10). Somewhat interestingly, by investigating how to utilize pre-trained models within the NeFL, we found that Vision Transformers (ViTs) hold promise in addressing both system and statistical heterogeneity.		</p>
		</p>
		</div>
	</div>
</div>

<h2> Previous Work </h2>

<div class="project">
	<div class="project-thumb"><img src="/photos/nefl.png" alt="stroke subject demonstrating hand closing and opening while wearing device"></div>
	<div class="project-content">
		<h3>NeFL: Nested Federated Learning for Heterogeneous Clients</h3>
		<span style="color:#767676">Honggu Kang, </span><strong>Seohyeon Cha</strong><span style="color:#767676">, Jinwoo Shin, Jongmyeong Lee, and Joonhyuk Kang</span><cbr>
		Under-review in NeurIPS 2023. 
		<div>
		<a href="https://openreview.net/pdf?id=hldhaq2P2z">paper</a> / 
		<a id="show_cha2023_nefl" href="#show_cha2023_nefl" class="hide">abstract (+)</a>
		<a id="hide_cha2023_nefl" href="#hide_cha2023_nefl" class="show">abstract (-)</a>
		<p class="details">
			Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.01% improvement on CIFAR-10). Somewhat interestingly, by investigating how to utilize pre-trained models within the NeFL, we found that Vision Transformers (ViTs) hold promise in addressing both system and statistical heterogeneity.		</p>
		</p>
		</div>
	</div>
</div>

<div class="project">
	<div class="project-thumb"><img src="/photos/its.png" alt="stroke subject demonstrating hand closing and opening while wearing device"></div>
	<div class="project-content">
		<h3>Intelligent Surface-aided Transmitarray Antenna in mmWave Communication System with Historical Channel Observation</h3>
		<strong>Seohyeon Cha</strong><span style="color:#767676">, Sanghyuk Kim, Jiwan Seo, and Joonhyuk Kang</span><cbr>
		In IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia), 2022. 
		<div>
		<a href="https://ieeexplore.ieee.org/document/9954733">paper</a> / 
		<a id="show_cha2022_its" href="#show_cha2022_its" class="hide">abstract (+)</a>
		<a id="hide_cha2022_its" href="#hide_cha2022_its" class="show">abstract (-)</a>
		<p class="details">
			In this paper, we study an intelligent surface-aided transmit-array antenna architecture which deploys intelligent transmitting surface (ITS) near the active antenna in mmWave downlink communication system. We aim to maximize the average achievable rate at the user by optimizing the phase shift matrix of ITS. To overcome the difficulty of acquiring channel state information (CSI), we propose a stochastic gradient descent (SGD)-based algorithm that requires only historical channel observations. Simulation results show that our proposed scheme with single active antenna and ITS outperforms conventional MISO system.
		</p>
		</div>
	</div>
</div>


<footer class="footer" style="vertical-align:bottom;text-align:left;margin-top:50px;">
	<hr>
    <div>kaitjgus<span style="font-size:9pt;padding-right:1px;padding-left:1px;"><i alt="at sign" class="fas fa-at"></i></span>kaist.ac.kr</div>
    <div>Last Updated:&nbsp; <time datetime="{{ site.time | date_to_xmlschema }}">{{ site.time | date: '%m-%d-%Y' }}</time></div>    
</footer>
