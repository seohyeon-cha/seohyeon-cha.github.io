---
layout: page
title: Research
---

<h2> Conformal Preidction </h2>
<p>
	My main focus is conformal prediction with Bayesian models, especially in graph dataset. 
</p>

<div class="project">
<!-- 	<div class="project-thumb"><img src="/photos/thumb.jpg" alt="stroke subject demonstrating hand closing and opening while wearing device"></div> -->
	<div class="project-content">
		<h3>NeFL: Nested Federated Learning for Heterogeneous Clients</h3>
		<span style="color:#767676">Honggu Kang, </span><strong>Seohyeon Cha</strong><span style="color:#767676">Jinwoo Shin, Jongmyeong Lee, and Joonhyuk Kang</span><cbr>
		Under-review in NeurIPS 2023. 
		<div>
		<a href="https://openreview.net/pdf?id=hldhaq2P2z">paper</a> / 
		<a id="show_cha2023_nefl" href="#show_cha2023_nefl" class="hide">abstract (+)</a>
		<a id="hide_cha2023_nefl" href="#hide_cha2023_nefl" class="show">abstract (-)</a>
		<p class="details">
			Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.01% improvement on CIFAR-10). Somewhat interestingly, by investigating how to utilize pre-trained models within the NeFL, we found that Vision Transformers (ViTs) hold promise in addressing both system and statistical heterogeneity.		</p>
		</p>
		</div>
	</div>
</div>

<h2> Previous Work </h2>

<div class="project">
	<div class="project-thumb"><img src="/photos/nefl.png" alt="stroke subject demonstrating hand closing and opening while wearing device"></div>
	<div class="project-content">
		<h3>NeFL: Nested Federated Learning for Heterogeneous Clients</h3>
		<span style="color:#767676">Honggu Kang, </span><strong>Seohyeon Cha</strong><span style="color:#767676">Jinwoo Shin, Jongmyeong Lee, and Joonhyuk Kang</span><cbr>
		Under-review in NeurIPS 2023. 
		<div>
		<a href="https://openreview.net/pdf?id=hldhaq2P2z">paper</a> / 
		<a id="show_cha2023_nefl" href="#show_cha2023_nefl" class="hide">abstract (+)</a>
		<a id="hide_cha2023_nefl" href="#hide_cha2023_nefl" class="show">abstract (-)</a>
		<p class="details">
			Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.01% improvement on CIFAR-10). Somewhat interestingly, by investigating how to utilize pre-trained models within the NeFL, we found that Vision Transformers (ViTs) hold promise in addressing both system and statistical heterogeneity.		</p>
		</p>
		</div>
	</div>
</div>

<div class="project">
	<div class="project-thumb"><img src="/photos/nefl.png" alt="stroke subject demonstrating hand closing and opening while wearing device"></div>
	<div class="project-content">
		<h3>NeFL: Nested Federated Learning for Heterogeneous Clients</h3>
		<span style="color:#767676">Honggu Kang, </span><strong>Seohyeon Cha</strong><span style="color:#767676">Jinwoo Shin, Jongmyeong Lee, and Joonhyuk Kang</span><cbr>
		Under-review in NeurIPS 2023. 
		<div>
		<a href="https://openreview.net/pdf?id=hldhaq2P2z">paper</a> / 
		<a id="show_cha2023_nefl" href="#show_cha2023_nefl" class="hide">abstract (+)</a>
		<a id="hide_cha2023_nefl" href="#hide_cha2023_nefl" class="show">abstract (-)</a>
		<p class="details">
			Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.01% improvement on CIFAR-10). Somewhat interestingly, by investigating how to utilize pre-trained models within the NeFL, we found that Vision Transformers (ViTs) hold promise in addressing both system and statistical heterogeneity.		</p>
		</p>
		</div>
	</div>
</div>


<footer class="footer" style="vertical-align:bottom;text-align:left;margin-top:50px;">
	<hr>
    <div>kaitjgus<span style="font-size:9pt;padding-right:1px;padding-left:1px;"><i alt="at sign" class="fas fa-at"></i></span>kaist.ac.kr</div>
    <div>Last Updated:&nbsp; <time datetime="{{ site.time | date_to_xmlschema }}">{{ site.time | date: '%m-%d-%Y' }}</time></div>    
</footer>
